version: '3.7'
services:

# Confluent kafka stack
  zk-1:
    image: confluentinc/cp-zookeeper:${CONFLUENT_VERSION}
    hostname: zk-1
    container_name: zk-1
    restart: on-failure
    volumes:
      - data-zk-log-1:/var/lib/zookeeper/log
      - data-zk-data-1:/var/lib/zookeeper/data
    networks:
      - elastic
    environment:
      - ZOOKEEPER_SERVER_ID=1
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
      - ZOOKEEPER_INIT_LIMIT=5
      - ZOOKEEPER_SYNC_LIMIT=2
      - ZOOKEEPER_SERVERS=zk-1:2888:3888;zk-2:2888:3888;zk-3:2888:3888
      - KAFKA_OPTS=-Xms128m -Xmx128m

  zk-2:
    image: confluentinc/cp-zookeeper:${CONFLUENT_VERSION}
    hostname: zk-2
    container_name: zk-2
    restart: on-failure
    volumes:
      - data-zk-log-2:/var/lib/zookeeper/log
      - data-zk-data-2:/var/lib/zookeeper/data
    networks:
      - elastic
    environment:
      - ZOOKEEPER_SERVER_ID=2
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
      - ZOOKEEPER_INIT_LIMIT=5
      - ZOOKEEPER_SYNC_LIMIT=2
      - ZOOKEEPER_SERVERS=zk-1:2888:3888;zk-2:2888:3888;zk-3:2888:3888
      - KAFKA_OPTS=-Xms128m -Xmx128m

  zk-3:
    image: confluentinc/cp-zookeeper:${CONFLUENT_VERSION}
    hostname: zk-3
    container_name: zk-3
    restart: on-failure
    volumes:
      - data-zk-log-3:/var/lib/zookeeper/log
      - data-zk-data-3:/var/lib/zookeeper/data
    networks:
      - elastic
    environment:
      - ZOOKEEPER_SERVER_ID=3
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
      - ZOOKEEPER_INIT_LIMIT=5
      - ZOOKEEPER_SYNC_LIMIT=2
      - ZOOKEEPER_SERVERS=zk-1:2888:3888;zk-2:2888:3888;zk-3:2888:3888
      - KAFKA_OPTS=-Xms128m -Xmx128m
      
  create-admin-user:
    image: "ianitrix/kafka:${CONFLUENT_VERSION}"
    container_name: create-admin-user
    depends_on:
      - zk-1
    entrypoint: "kafka-configs --zookeeper zk-1:2181 --alter --add-config SCRAM-SHA-512='[password=admin-secret]' --entity-type users --entity-name admin"
    restart: on-failure
    networks:
      - elastic

  kafka-1:
    image: confluentinc/cp-kafka:${CONFLUENT_VERSION}
    hostname: kafka-1
    container_name: kafka-1
    restart: on-failure
    networks:
      - elastic
    depends_on:
      - zk-1
      - zk-2
      - zk-3
    volumes:
      - data-kafka-1:/var/lib/kafka/data
    environment:
      - KAFKA_BROKER_ID=101
      - KAFKA_ZOOKEEPER_CONNECT=zk-1:2181,zk-2:2181,zk-3:2181
      - KAFKA_ADVERTISED_LISTENERS=SECURE://kafka-1:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=SECURE:SASL_PLAINTEXT
      - KAFKA_LISTENERS=SECURE://:9092
      - KAFKA_SASL_ENABLED_MECHANISMS=SCRAM-SHA-512
      - KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL=SCRAM-SHA-512
      - KAFKA_INTER_BROKER_LISTENER_NAME=SECURE
      - "KAFKA_LISTENER_NAME_SECURE_SCRAM___sha___512_sasl_jaas_config=org.apache.kafka.common.security.scram.ScramLoginModule required username=admin password=admin-secret;"
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_DELETE_TOPIC_ENABLE=true
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_CONFLUENT_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_OPTS=-Xms256m -Xmx256m
      - KAFKA_AUTHORIZER_CLASS_NAME=kafka.security.authorizer.AclAuthorizer
      - KAFKA_SUPER_USERS=User:admin
    healthcheck:
      test: nc -z localhost 9092
      interval: 2s
      timeout: 2s
      retries: 3
      start_period: 2s

  kafka-2:
    image: confluentinc/cp-kafka:${CONFLUENT_VERSION}
    hostname: kafka-2
    container_name: kafka-2
    restart: on-failure
    networks:
      - elastic
    depends_on:
      - zk-1
      - zk-2
      - zk-3
    volumes:
      - data-kafka-2:/var/lib/kafka/data
    environment:
      - KAFKA_BROKER_ID=102
      - KAFKA_ZOOKEEPER_CONNECT=zk-1:2181,zk-2:2181,zk-3:2181
      - KAFKA_ADVERTISED_LISTENERS=SECURE://kafka-2:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=SECURE:SASL_PLAINTEXT
      - KAFKA_LISTENERS=SECURE://:9092
      - KAFKA_SASL_ENABLED_MECHANISMS=SCRAM-SHA-512
      - KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL=SCRAM-SHA-512
      - KAFKA_INTER_BROKER_LISTENER_NAME=SECURE
      - "KAFKA_LISTENER_NAME_SECURE_SCRAM___sha___512_sasl_jaas_config=org.apache.kafka.common.security.scram.ScramLoginModule required username=admin password=admin-secret;"
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_DELETE_TOPIC_ENABLE=true
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_CONFLUENT_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_OPTS=-Xms256m -Xmx256m
      - KAFKA_AUTHORIZER_CLASS_NAME=kafka.security.authorizer.AclAuthorizer
      - KAFKA_SUPER_USERS=User:admin
    healthcheck:
      test: nc -z localhost 9092
      interval: 2s
      timeout: 2s
      retries: 3
      start_period: 2s

  kafka-3:
    image: confluentinc/cp-kafka:${CONFLUENT_VERSION}
    hostname: kafka-3
    container_name: kafka-3
    restart: on-failure
    networks:
      - elastic
    depends_on:
      - zk-1
      - zk-2
      - zk-3
    volumes:
      - data-kafka-3:/var/lib/kafka/data
    environment:
      - KAFKA_BROKER_ID=103
      - KAFKA_ZOOKEEPER_CONNECT=zk-1:2181,zk-2:2181,zk-3:2181
      - KAFKA_ADVERTISED_LISTENERS=SECURE://kafka-3:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=SECURE:SASL_PLAINTEXT
      - KAFKA_LISTENERS=SECURE://:9092
      - KAFKA_SASL_ENABLED_MECHANISMS=SCRAM-SHA-512
      - KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL=SCRAM-SHA-512
      - KAFKA_INTER_BROKER_LISTENER_NAME=SECURE
      - "KAFKA_LISTENER_NAME_SECURE_SCRAM___sha___512_sasl_jaas_config=org.apache.kafka.common.security.scram.ScramLoginModule required username=admin password=admin-secret;"
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_DELETE_TOPIC_ENABLE=true
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_CONFLUENT_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_OPTS=-Xms256m -Xmx256m
      - KAFKA_AUTHORIZER_CLASS_NAME=kafka.security.authorizer.AclAuthorizer
      - KAFKA_SUPER_USERS=User:admin
    healthcheck:
      test: nc -z localhost 9092
      interval: 2s
      timeout: 2s
      retries: 3
      start_period: 2s

  kafka-connect-1:
    image: "confluentinc/cp-kafka-connect:${CONFLUENT_VERSION}"
    hostname: kafka-connect-1
    container_name: kafka-connect-1
    ports:
      - 8083:8083
    networks:
      - elastic
    depends_on:
      - kafka-1
    healthcheck:
      test: test `curl -s -o /dev/null -w "%{http_code}" http://localhost:8083/connectors` = 200
      interval: 2s
      timeout: 2s
      retries: 10
      start_period: 2s
    environment:
      - CONNECT_BOOTSTRAP_SERVERS=kafka-1:9092,kafka-2:9092,kafka-3:9092
      - CONNECT_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - CONNECT_SASL_MECHANISM=SCRAM-SHA-512
      - "CONNECT_SASL_JAAS_CONFIG=org.apache.kafka.common.security.scram.ScramLoginModule required username=admin password=admin-secret;"
      - CONNECT_CONSUMER_SASL_MECHANISM=SCRAM-SHA-512
      - CONNECT_PRODUCER_SASL_MECHANISM=SCRAM-SHA-512
      - CONNECT_CONSUMER_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - CONNECT_PRODUCER_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - CONNECT_REST_PORT=8083
      - CONNECT_GROUP_ID=connect
      - CONNECT_CONFIG_STORAGE_TOPIC=_connect-config
      - CONNECT_OFFSET_STORAGE_TOPIC=_connect-offsets
      - CONNECT_STATUS_STORAGE_TOPIC=_connect-status
      - CONNECT_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE=false
      - CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE=false
      - CONNECT_INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_REST_ADVERTISED_HOST_NAME=kafka-connect-1
      - CONNECT_PLUGIN_PATH=/confluent-${CONFLUENT_VERSION}/share/java
      - CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY=All
    restart: on-failure

  schema-registry:
    image: confluentinc/cp-schema-registry:${CONFLUENT_VERSION}
    hostname: schema-registry
    container_name: schema-registry
    networks:
      - elastic
    environment:
      - SCHEMA_REGISTRY_HOST_NAME=schema-registry
      - SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=kafka-1:9092,kafka-2:9092,kafka-3:9092
      - SCHEMA_REGISTRY_LISTENERS=http://schema-registry:8081
      - SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - SCHEMA_REGISTRY_KAFKASTORE_SASL_MECHANISM=SCRAM-SHA-512
      - "SCHEMA_REGISTRY_KAFKASTORE_SASL_JAAS_CONFIG=org.apache.kafka.common.security.scram.ScramLoginModule required username=admin password=admin-secret;"


  akhq:
    image: tchiotludo/akhq:0.16.0
    container_name: akhq
    environment:
     AKHQ_CONFIGURATION: |
        micronaut:
         server:
           cors:
             enabled: true
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "kafka-1:9092,kafka-2:9092,kafka-3:9092"
                security.protocol: SASL_PLAINTEXT
                sasl.mechanism: SCRAM-SHA-512
                sasl.jaas.config: org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";
              schema-registry:
                url: "http://schema-registry:8081"
              connect:
                - name: connect
                  url: "http://kafka-connect-1:8083"
    ports:
    - 8085:8080
    networks:
    - elastic

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_VERSION}
    environment:
    - bootstrap.memory_lock=true
    - cluster.name=docker-cluster
    - cluster.routing.allocation.disk.threshold_enabled=false
    - discovery.type=single-node
    - ES_JAVA_OPTS=-XX:UseAVX=2 -Xms1g -Xmx1g
    ulimits:
      memlock:
        hard: -1
        soft: -1
    volumes:
    - esdata:/usr/share/elasticsearch/data
    ports:
    - 9200:9200
    networks:
    - elastic
    healthcheck:
      interval: 20s
      retries: 10
      test: curl -s http://localhost:9200/_cluster/health | grep -vq '"status":"red"'

  kibana:
    image: docker.elastic.co/kibana/kibana:${ELASTICSEARCH_VERSION}
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
    - 5601:5601
    networks:
    - elastic
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status

  filebeat:
    image: docker.elastic.co/beats/filebeat:${ELASTICSEARCH_VERSION}
    user: root
    networks:
      - elastic
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker/containers/:/var/lib/docker/containers/:ro
      - ./config/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
    environment:
      - ELASTICSEARCH_HOST=http://elasticsearch:9200
      - KIBANA_HOST=kibana
    command: ["--strict.perms=false"]
    restart: on-failure
    
  # Prometheus
  prometheus:
    image: prom/prometheus:${PROMETHEUS_VERSION}
    volumes:
      - ./config/prometheus.yml:/prometheus.yml:ro
    command:
      - '--config.file=/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: on-failure
    ports:
     - "9090:9090"
    networks:
      - elastic

volumes:
  esdata:
    driver: local
  data-zk-log-1:
  data-zk-log-2:
  data-zk-log-3:
  data-zk-data-1:
  data-zk-data-2:
  data-zk-data-3:
  data-kafka-1:
  data-kafka-2:
  data-kafka-3:


networks:
  elastic:
    driver: bridge